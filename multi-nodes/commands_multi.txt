Steps:
1. Run a master node and a worker node, both with same VM specs
2. SSH into master node and the worker nodes, install microk8s in both of them
3. Start microk8s on both servers

// Start microk8s
sudo microk8s start

4. Enable required addons (dashboard, metrics-server)

// Enable both addons
sudo microk8s enable dns
sudo microk8s enable dashboard
sudo microk8s enable metrics-server

5. Add and install a helm chart of a hadoop application designed to be executed on k8s into the local repo

// Add and install helm chart to repo
sudo microk8s helm repo add pfisterer-hadoop https://pfisterer.github.io/apache-hadoop-helm/
sudo microk8s helm install hadoop pfisterer-hadoop/hadoop

6. Connect the second node to the cluster

// Get the master node's add-node command
sudo microk8s add-node

7. Run the command provided by the master node to connect the worker node

8. Add a set of data node and node manager to the cluster
sudo microk8s helm upgrade hadoop --set yarn.dataNode.replicas=2,yarn.nodeManager.replicas=2 pfisterer-hadoop/hadoop

9. Expose the resourcemanager's port

// Expose resource manager port
sudo microk8s kubectl port-forward -n default hadoop-hadoop-yarn-rm-0 8088:8088 --address 0.0.0.0

10. View the hadoop dashboard

// Create hdfs input folder and add the input txt data into the folder
hdfs dfs -mkdir -p input
hdfs dfs -put ./input/* input

// Start mapreduce (choose mapred)
mapred streaming -files mapper.py,reducer.py -mapper "python mapper.py" -reducer "python reducer.py" -input input -output output

// Remove the output folder in hdfs
hadoop fs -rm -r output

// List the output files in hdfs
hdfs dfs -ls output

// View the output file
hdfs dfs -cat output/*

// Namenode UI http://localhost:9870/
// ResourceManager UI http://localhost:8088/